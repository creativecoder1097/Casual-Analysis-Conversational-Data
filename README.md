# Casual-Analysis-Conversational-Data

ğŸ“Œ Problem Overview

Customer support conversations often end in outcomes such as escalation, complaints, fraud investigation, or refunds.
While these outcomes are known, the reasons behind them are not always clear.

This project focuses on grounded explanation and reasoning over conversational data, rather than prediction.

The goal is to:

analyze multi-turn customerâ€“agent conversations

identify why certain outcomes occur

support explanations using verbatim dialogue evidence

ğŸ“‚ Dataset

File: Conversational_Transcript_Dataset.json

Total conversations: 5,037

Domains: Banking, Healthcare, Telecom, E-commerce, Insurance

Each conversation contains:

transcript_id

domain

intent (final outcome)

reason_for_call

ordered conversation turns (Agent / Customer)

ğŸ§  Approach Overview

We implemented a rule-based, explainable reasoning pipeline designed for faithfulness and interpretability, which aligns with the hackathon evaluation criteria.

The system focuses on:

retrieving relevant conversations

extracting escalation and issue signals from dialogue

generating human-readable explanations

grounding every claim with transcript evidence

No hallucination or synthetic text is introduced.

ğŸ§ª Task 1 â€“ Single-turn Explanation
Objective

Given a single analytical question (e.g. â€œWhy do customers escalate calls in banking conversations?â€), generate a grounded explanation.

Methodology

Infer intent from the question using keyword matching

Retrieve relevant conversations based on intent

Extract customer escalation signals (e.g. requests for managers, complaints)

Aggregate common linguistic patterns

Generate a concise explanation supported by representative transcript IDs

Output

Natural language explanation

Verbatim dialogue snippets

3â€“5 representative transcript IDs as evidence

This ensures:

Faithfulness â€“ all claims are grounded

Clarity â€“ concise, human-readable output

Relevance â€“ no unnecessary data dumping

ğŸ”„ Task 2 â€“ Multi-turn Follow-up Reasoning
Objective

Handle follow-up questions while maintaining context, consistency, and evidence reuse.

Key Feature: Conversational Memory

A memory object stores:

inferred intent

retrieved conversations

transcript IDs used as evidence

Behavior

For follow-up questions, previously analyzed conversations are reused

For new topics, fresh retrieval is performed

Analysis focus can shift (customer behavior â†’ agent behavior â†’ domain-level patterns)

Transcript IDs remain consistent across turns

Example Follow-ups

â€œWhich agent responses made the situation worse?â€

â€œIs this behavior common across other domains?â€

ğŸ—ï¸ Project Structure
.
â”œâ”€â”€ Conversational_Transcript_Dataset.json
â”œâ”€â”€ Task1_Task2.ipynb
â”œâ”€â”€ predictive powerhouse.ipynb
â”œâ”€â”€ task1_queries_outputs.csv
â””â”€â”€ README.md

âš™ï¸ How to Run

Install dependencies:

pip install pandas


Open the notebook:

jupyter notebook Task1_Task2.ipynb


Run cells sequentially to execute:

Task 1 (single-question explanation)

Task 2 (multi-turn follow-up reasoning)

âœ… Evaluation Alignment

This solution is designed to maximize:

Faithfulness â€“ no hallucinated content

ID Recall â€“ correct and relevant transcript IDs

Relevance â€“ answers directly address the question

Explainability â€“ easy to interpret and justify

The approach intentionally prioritizes clarity and grounding over complex modeling.

ğŸš€ Final Notes

This is an analysis and explanation system, not a prediction model

Representative transcript IDs are intentionally used instead of exhaustive listing

The system is robust, transparent, and judge-defensible

ğŸ‘¥ Team

The Outliers

(Sk Aziz Jalaluddin, Soham Swain, Shubham Acharya)
